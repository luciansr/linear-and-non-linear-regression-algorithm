{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4I-NGfBLdmL"
   },
   "source": [
    "## Inteligência Computacional II - Trabalho 2\n",
    "\n",
    "Prof. Carlos Eduardo Pedreira  \n",
    "PESC/COPPE/UFRJ  \n",
    "Trabalho Prático I  \n",
    "\n",
    "\n",
    "O Trabalho Prático II visa implementar os exercícios computacionais descritos no Homework #2 disponibilizado em:  \n",
    "https://work.caltech.edu/homework/hw2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "### Regressão linear [ref.](https://www.dicionariofinanceiro.com/regressao-linear/)\n",
    "\n",
    "A regressão linear é uma metodologia desenvolvida a partir da estatística e da econometria, para analisar a relação entre variáveis formadas em um dado e seus resultados.\n",
    "\n",
    "A relação parte de uma variável de interesse (dependente) com outras que a possam influenciar, por exemplo, analisar a venda de um produto relacionada ao crescimento populacional de um país.\n",
    "\n",
    "Com os resultados obtidos a regressão linear visualiza as maiores tendências que as variáveis analisadas apresentam. A regressão consiste em modelar na estatística, os valores que se quer observar.\n",
    "\n",
    "Esta regressão é linear quando os acontecimentos observados na função apresentam um formato em linha reta, como na imagem a seguir:\n",
    "\n",
    "![Regressão Linear](normdist-regression.jpg)\n",
    "\n",
    "A análise de regressão é mais útil quando apresentada em um diagrama de dispersão, muito utilizados em economia, administração de empresas e indústrias, ou também, os dados de um país.\n",
    "\n",
    "Os acontecimentos quando não são combinados em forma linear é conhecida como regressão não-linear, e em seus gráficos, a tendência se apresenta em outros formatos.\n",
    "\n",
    "Regressão linear simples\n",
    "A regressão linear é simples quando são observadas apenas duas variáveis, normalmente X e Y, sendo que uma é dependente (Y) e será a função de outra que se comporta independente (X).\n",
    "\n",
    "A regressão linear simples é analisada através da fórmula $y = ax + b$, onde $b$ é o coeficiente linear e $a$ é o coeficiente angular.\n",
    "\n",
    "Nos casos em que as variáveis são mais do que duas passa a se chamar regressão linear múltipla e a demonstração visual a partir de um gráfico se torna mais complexa.\n",
    "\n",
    "### Regressão linear para classificação\n",
    "\n",
    "A diferença quando se usa uma regressão linear para classificação é que assim como no algoritmo [PLA](https://github.com/luciansr/simple-perceptron-pla-algorithm/blob/master/src/simple-perceptron.ipynb) o objetivo é classificar pontos que tem seus valores como $+1$ e $-1$. Para isso teremos que mapear os dados que inicialmente estão no $R_2$ e gerar os valores de $y_n$ com valor $+1$ caso estejam acima da função target, e $-1$ caso estejam abaixo da função target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kOhGo1U6YHpR"
   },
   "source": [
    "## 1. Criando uma função target aleatória"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para começar o experimento, uma função target aleatória será gerada. Essa função será usada como base para geração de nossa base de treinamento e testes. Esta função será gerada como um vetor com ```1 linha``` e ```2 colunas``` já que estamos trabalhando no $R_2$ e o objetivo é gerar uma target function no formato $y = ax + b$ onde o vetor representará $[a, b]$. \n",
    "\n",
    "Para gerar um função target aleatória será utilizado o método ```np.random.rand(<dimensions>)``` onde pode-se definir quais as dimensoes da matriz gerada. A função rand da biblioteca ```numpy``` gera números aleatórios entre $[0, 1)$. Para obter-se uma função com valores entre $-1$ e $1$ multiplica-se por $2$ e subtrai-se $1$ para que $[0,1) \\to [-1,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1533179229996,
     "user": {
      "displayName": "Lucian Sturião",
      "photoUrl": "//lh6.googleusercontent.com/-EQEjkTuE5bo/AAAAAAAAAAI/AAAAAAAAhDs/hf44b7V3N-U/s50-c-k-no/photo.jpg",
      "userId": "109847505924129083379"
     },
     "user_tz": 180
    },
    "id": "jRJsQogakhsn",
    "outputId": "a8f87bd4-4bad-40cf-bada-1e6afd5cc54f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targetFuncion = [ 0.47691948 -0.3689029 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "    \n",
    "targetFunction = np.random.rand(2)*2 - 1\n",
    "\n",
    "print('targetFuncion = ' + str(targetFunction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E sendo a reta $y = ax + b$ e a targetFunction = $[a, b]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targetFunction A = 0.476919483907\n",
      "targetFunction B = -0.368902901986\n"
     ]
    }
   ],
   "source": [
    "targetFunctionA = targetFunction[0]\n",
    "targetFunctionB = targetFunction[1]\n",
    "\n",
    "print('targetFunction A = ' + str(targetFunctionA))\n",
    "print('targetFunction B = ' + str(targetFunctionB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se então visualizar esta target function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD3CAYAAAD/oDhxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111d0b978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def xInTargetFunction(x):\n",
    "    return targetFunctionA*x + targetFunctionB\n",
    "\n",
    "xValues = np.linspace(-1, 1, 100)\n",
    "yValues = np.array([])\n",
    "\n",
    "for x in xValues:\n",
    "    yValues = np.append(yValues, xInTargetFunction(x))\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.plot(xValues, yValues)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1,1])\n",
    "axes.set_ylim([-1,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnF6H3R_Y8Wb"
   },
   "source": [
    "## 2. Gerando Dados a partir da função de geração (target)\n",
    "\n",
    "Dada a função gerada, pode-se agora gerar pontos aleatórios e classificá-los a partir da mesma para gerar pontos de treinamento. A ideia aqui é gerar o conjunto $X$ matriz $(100,2)$ com pontos aleatórios $[-1,1]$ $x$ $[-1,1]$ (será utilizado o mesmo método ```numpy.random.rand``` utilizado anteriormente), sendo cada $x_n = [x,y]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.74561938  0.78149199]\n",
      " [-0.15144173  0.05069513]\n",
      " [ 0.43179354  0.16523443]\n",
      " [-0.32367427 -0.927555  ]\n",
      " [ 0.73513033 -0.93828663]\n",
      " [-0.63197967  0.33517052]\n",
      " [-0.30591315 -0.75453632]\n",
      " [-0.32666891 -0.1898323 ]\n",
      " [-0.95862952  0.08061957]\n",
      " [ 0.65187155  0.22878186]] \n",
      "[...]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "NUMBER_OF_POINTS = 100\n",
    "\n",
    "dataX = np.random.rand(NUMBER_OF_POINTS,2)*2 - 1\n",
    "\n",
    "# mostrando somente primeiros 10 itens\n",
    "print(str(dataX[:10]) + \" \\n[...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classificá-los usando a função ```classifica``` que retorna $1$ caso o ponto esteja acima da reta da targetFunction, e 0 caso contrário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificaReg(linha, pesos):\n",
    "  resultado = pesos[0]*linha[0] + pesos[1]\n",
    "  \n",
    "  if (linha[1] >= resultado):\n",
    "    return 1.0\n",
    "  \n",
    "  return -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E gerar as labels de resultado $Y$ a partir da classificação de $X$ com a função target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. -1. -1.  1. -1.  1.  1.  1.] [...]\n"
     ]
    }
   ],
   "source": [
    "dataY = np.array([])\n",
    "\n",
    "for x in dataX:\n",
    "    dataY = np.append(dataY, classificaReg(x, targetFunction))\n",
    "\n",
    "# mostrando somente primeiros 10 itens\n",
    "print(str(dataY[:10]) + \" [...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora pode-se ver o resultado no gráfico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD3CAYAAAD/oDhxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1121e9cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.plot(xValues, yValues)\n",
    "\n",
    "# cores a ser mostradas\n",
    "yColors = np.array([])\n",
    "\n",
    "for y in dataY:\n",
    "    yColors = np.append(yColors, 'b' if y == -1 else 'r')\n",
    "\n",
    "plt.plot(xValues, yValues)\n",
    "plt.scatter(dataX.T[0], dataX.T[1],c=yColors)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1,1])\n",
    "axes.set_ylim([-1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que na verdade estamos observando um gráfico 2D, mas na verdade os pontos vermelhos tem valor $+1$ enquanto os pontos em azul tem valor $-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZYrnUKgZJk7"
   },
   "source": [
    "## 3. Implementando a Regressão Linear\n",
    "\n",
    "O algoritmo de aprendizado da Regressão é muito simples. Vamos aplicar a fórmula:   \n",
    "\n",
    "$$w = X^{\\dagger}y$$ onde $$X^{\\dagger} = (X^TX)^{-1}X^T$$ e $X^{\\dagger}$ é a pseudo-inversa de $X$.\n",
    "\n",
    "Será usada a função ```np.linalg.pinv(<X>)``` que calcula a pseudo-inversa.\n",
    "\n",
    "Algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ev2Jv4ZFcIJD"
   },
   "outputs": [],
   "source": [
    "def regressionFit(X, y):\n",
    "    # cria com um vetor inicial por causa da funcao vstack\n",
    "    trainingX = np.array([0,0,0])\n",
    "    \n",
    "    for x in X:\n",
    "        trainingX = np.vstack([trainingX, np.array([x[0], x[1], 1])])\n",
    "    \n",
    "    # usa o X treinamento sem considerar a primeira linha\n",
    "    pseudoInvX = np.linalg.pinv(trainingX[1:])\n",
    "    return pseudoInvX.dot(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma outra função que será usada é a função ```regressionPredict(w, x)```: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionPredict(w, x):\n",
    "    regressionValue = w.T.dot(np.array([x[0], x[1], 1]))\n",
    "    return 1.0 if regressionValue >= 0 else -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fittedW:\n",
      "[-0.52643457  1.15600334  0.37039235]\n"
     ]
    }
   ],
   "source": [
    "fittedW = regressionFit(dataX, dataY)\n",
    "\n",
    "print('fittedW:')\n",
    "print(fittedW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Uma observação que vale a pena ser feita é que a regressão linear não atinge 100% de acertos nos dados de entrada, assim com o PLA. Ou seja, $E_{in}$ não é $0%$. Será utilizada a seguinte função para testar o erro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculaErro(w, X, Y):\n",
    "    erro = 0\n",
    "    index = 0\n",
    "    for x in X:        \n",
    "        if regressionPredict(w, x) != Y[index]:\n",
    "            erro += 1\n",
    "            \n",
    "        index += 1\n",
    "    return erro/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nosso exemplo o $E_{in}$ foi de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro no conjunto de treinamento (Ein) é: 5.0%\n"
     ]
    }
   ],
   "source": [
    "Ein = calculaErro(fittedW, dataX, dataY)\n",
    "print('Erro no conjunto de treinamento (Ein) é: ' + str(Ein*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular um $E_{out}$ devemos gerar dados que não foram usados para treinar $w$ e criar um nodo vetor $Y$ a partir da targetFunction e utilizar nossa função com o vetor $w$ gerado anteriormente e calcular o erro. \n",
    "\n",
    "Para isso será Utilizada a seguinte função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculaErroForaDaAmostra(w, tempFunction, NUMBER_OF_POINTS):\n",
    "    tempX = np.random.rand(NUMBER_OF_POINTS,2)*2 - 1    \n",
    "    tempY = np.array([])\n",
    "\n",
    "    for x in tempX:\n",
    "        tempY = np.append(tempY, classificaReg(x, tempFunction))\n",
    "    \n",
    "    return calculaErro(w, tempX, tempY)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então para um conjunto de teste de $100$ registros, teríamos um $E_{out}$ de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro no conjunto de teste (Eout) é: 0.0%\n"
     ]
    }
   ],
   "source": [
    "Eout = calculaErroForaDaAmostra(fittedW, targetFunction, 100)\n",
    "print('Erro no conjunto de teste (Eout) é: ' + str(Eout*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "### Regressão Linear\n",
    "\n",
    "\n",
    "Nestes problemas nós vamos explorar como a Regressão Linear para classificação trabalha. Da mesma maneira com uso do Algoritmo de Aprendizagem Perceptron no Homework #1, você vai criar a sua própria função target (alvo) $f$ e o conjunto de dados $D$. Utilize $d = 2$ para que você possa visualizar o problema, e assuma $X = [-1,1] × [-1,1]$ com probabilidade uniforme selecionando cada $x ∈ X$ . Em cada execução, escolha uma reta aleatória no plano como sua função target $f$ ( faça isso selecionando dois pontos aleatórios, uniformemente distribuídos em $[-1,1] × [-1,1]$ e use a reta que passa entre eles), de forma que a reta mapeie $+1$ por um lado e $-1$ pelo outro. Escolha as entradas $x_n$ do conjunto de dados de pontos aleatórios (uniformemente em $X$), e avalie a função target em cada $x_n$ para encontrar a saída correspondente $y_n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolução\n",
    "\n",
    "Serão utilizadas as funções ```classifica``` e ```fit``` nas questões que é necessário testar usando PLA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifica(row, f):\n",
    "    soma = np.append(row, 1).dot(f)\n",
    "\n",
    "    if (soma >= 0.0):\n",
    "        return 1.0\n",
    "\n",
    "    return 0.0\n",
    "\n",
    "def fit(X, Y, w, lr = 1):\n",
    "    i = 0\n",
    "    tempW = w\n",
    "    while True:\n",
    "        changed = False\n",
    "        i = i+1\n",
    "        index = 0\n",
    "        for x in X:\n",
    "            pred = classifica(x, tempW)\n",
    "            exp = 1.0 if Y[index] > 0.0 else 0.0\n",
    "            #print(pred)\n",
    "            #print(exp)\n",
    "            #print(' ')\n",
    "            if pred != exp:\n",
    "                changed = True\n",
    "                tempW = tempW + lr * (exp-pred) * np.append(x, 1)\n",
    "            index +=1\n",
    "                \n",
    "        # shuffle como requisitado no enunciado        \n",
    "        np.random.shuffle(X)\n",
    "        if changed == False:\n",
    "            break\n",
    "\n",
    "    return tempW, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a função auxiliar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificaReg(linha, f):\n",
    "  resultado = f[0]*linha[0] + f[1]\n",
    "  \n",
    "  if (linha[1] >= resultado):\n",
    "    return 1.0\n",
    "  \n",
    "  return -1.0\n",
    "\n",
    "def regressionFit(X, y):\n",
    "    trainingX = np.array([0,0,0])\n",
    "    \n",
    "    for x in X:\n",
    "        trainingX = np.vstack([trainingX, np.array([x[0], x[1], 1])])\n",
    "        \n",
    "    pseudoInvX = np.linalg.pinv(trainingX[1:])\n",
    "    return pseudoInvX.dot(y)\n",
    "\n",
    "\n",
    "def regressionPredict(w, x):\n",
    "    regressionValue = w.T.dot(np.array([x[0], x[1], 1]))\n",
    "    return 1.0 if regressionValue >= 0 else -1.0\n",
    "\n",
    "\n",
    "# retorna \n",
    "#    1. Ein - probabilidade de erro entre y e w^t*x calculada empiricamente \n",
    "#       usando <nExperimentos> experimentos dentro do conjunto de treinamento\n",
    "#    2. Eout - probabilidade de erro entre y e w^t*x calculada empiricamente \n",
    "#       usando <nExperimentos> experimentos dentro do conjunto de testes\n",
    "#\n",
    "# parametros\n",
    "#   1. nPontos = Número de pontos gerados para cada experimento\n",
    "#   2. nExperimentos = número de experimentos executados no cálculo\n",
    "#\n",
    "def calculaRegression_e_PLA(nPontos, nExperimentos = 1000):\n",
    "    Eout = 0.0    \n",
    "    Ein = 0.0 \n",
    "    nIter = 0.0;\n",
    "    \n",
    "    for i in range(nExperimentos):\n",
    "\n",
    "        # gera funcao target F entre [-1, 1)\n",
    "        targetFunctionF = np.random.rand(2)*2 - 1\n",
    "\n",
    "        # gera N pontos\n",
    "        innerDataX = np.random.rand(nPontos,2)*2 - 1\n",
    "        \n",
    "        innerY = np.array([])\n",
    "\n",
    "        # gera pontos em y -1 ou +1\n",
    "        for x in innerDataX:\n",
    "            innerY = np.append(innerY, classificaReg(x, targetFunctionF))\n",
    "            \n",
    "        # fit em w e recebendo numero de iteracoes\n",
    "        fittedInnerW = regressionFit(innerDataX, innerY)\n",
    "        \n",
    "        #w = np.array([0,0,0])\n",
    "        \n",
    "        #print(fittedInnerW)\n",
    "        # fit em w usando PLA com W inicial gerado pela regressão\n",
    "        #innerIter = 0.0\n",
    "        fittedPLA_W, innerIter = fit(innerDataX, innerY, fittedInnerW)\n",
    "        #print(fittedPLA_W, innerIter)\n",
    "                \n",
    "        # gera N pontos para teste\n",
    "        innerTestDataX = np.random.rand(nPontos,2)*2 - 1\n",
    "        \n",
    "        # calculando numero de erros do conjunto de treinamento\n",
    "        errors = 0.0\n",
    "        for x in innerDataX:\n",
    "            predicted = regressionPredict(fittedInnerW, x)\n",
    "            expected =  classificaReg(x, targetFunctionF)\n",
    "            if predicted != expected:\n",
    "                errors = errors + 1.0\n",
    "        \n",
    "        innerEin = errors / nPontos;\n",
    "                        \n",
    "        # calculando numero de erros do conjunto de testes\n",
    "        errors = 0.0\n",
    "        for x in innerTestDataX:\n",
    "            predicted = regressionPredict(fittedInnerW, x)\n",
    "            expected =  classificaReg(x, targetFunctionF)\n",
    "            if predicted != expected:\n",
    "                errors = errors + 1.0\n",
    "        \n",
    "        innerEout = errors / nPontos;\n",
    "        \n",
    "        # adicionando Ein\n",
    "        Ein = Ein + innerEin\n",
    "        # adicionando Eout\n",
    "        Eout = Eout + innerEout\n",
    "        # n iteracoes\n",
    "        nIter = nIter + innerIter;\n",
    "            \n",
    "        \n",
    "    #end for\n",
    "    Eout = Eout/nExperimentos;\n",
    "    Ein = Ein/nExperimentos;\n",
    "    nIter = nIter/nExperimentos;\n",
    "    \n",
    "    return Ein, Eout, nIter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Utilize N = 100. Use Regressão  não Linear para encontrar g e avaliar $E_{in}$, a fração de pontos dentro da amostra que foram classificados incorretamente. Repita o experimento 1000 vezes e use o valor médio (guarde as g’s que serão usadas novamente no Problema 2). Qual é o valor médio aproximado de $E_{in}$? (aproximado é a opção que faz a expressão | sua resposta - dada opção | próxima a 0. Use esta definição aqui e sempre).\n",
    "\n",
    "a) $0$ b) $0.001$ c)**(x)** $0.01$ d) $0.1$ e) $0.5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Agora, gere 1000 novos pontos e os use para estimar o erro fora da amostra $E_{out}$ de g que você fez no Problema 1 (número de pontos classificados incorretamente / número total de pontos fora da amostra). Novamente, execute o experimento 1000 vezes e guarde a média. Qual é o valor médio aproximado de $E_{out}$?\n",
    "\n",
    "a) $0$ b) $0.001$ c)**(x)** $0.01$ d) $0.1$ e) $0.5$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ein = 0.04218000000000003\n",
      "Eout = 0.052470000000000065\n",
      "PLA Iter = 402.12415\n"
     ]
    }
   ],
   "source": [
    "N100Ein, N100Eout, N100Iter = calculaRegression_e_PLA(100)\n",
    "print('Ein = ' + str(N100Ein))\n",
    "print('Eout = ' + str(N100Eout))\n",
    "print('PLA Iter = ' + str(N100Iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Agora, utilize N = 10. Posteriormente, procurando os pesos usando Regressão Linear, os use como um vetor de pesos inicial para o Algoritmo de Aprendizagem Perceptron. Execute PLA até que convirja para o vetor final de pesos que separe completamente todos os pontos dentro da amostra. Entre as opções abaixo, qual é o valor mais próximo do número médio de iterações (mais de 1000 execuções) que o PLA leva para convergir? (Quando estiver implementando o PLA, escolha um ponto aleatório para o conjunto classificado incorretamente para cada iteração).\n",
    "\n",
    "a) $1$ b) $15$ c) $300$ **d)(x)** $5000$ e) $10000$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Não-Linear\n",
    "\n",
    "Nestes problemas, nós vamos novamente aplicar Regressão Linear para classificação. Considere a função target:\n",
    "\n",
    "$f(x_1, x_2) = sign(x^2_1 + x^2_2 − 0.6)$.\n",
    "\n",
    "Gere um conjunto de treinamento de $N = 1000$ pontos em $X = [-1,1]×[-1,1]$ com probabilidade uniforme escolhendo cada $x ∈ X$ . Gere um ruído simulado lançando o sinal de saída aleatoriamente selecionando $10%$ do subconjunto de treinamento gerado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Será utilizada uma versão modificada da anterior:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificaRegNLin(linha):\n",
    "  resultado = linha[0]**2 + linha[1]**2 - 0.6\n",
    "  \n",
    "  if (linha[1] >= resultado):\n",
    "    return 1.0\n",
    "  \n",
    "  return -1.0\n",
    "\n",
    "def regressionNLinFit(X, y):\n",
    "    trainingX = np.array([0,0,0])\n",
    "    \n",
    "    for x in X:\n",
    "        trainingX = np.vstack([trainingX, np.array([1, x[0], x[1]])])\n",
    "        \n",
    "    pseudoInvX = np.linalg.pinv(trainingX[1:])\n",
    "    return pseudoInvX.dot(y)\n",
    "\n",
    "\n",
    "def regressionNLinPredict(w, x):\n",
    "    regressionValue = w.T.dot(np.array([1, x[0], x[1]]))\n",
    "    return 1.0 if regressionValue >= 0 else -1.0\n",
    "\n",
    "def calculaRegressionNLin(nPontos, nExperimentos = 1000, quantidadeRuido = 10):\n",
    "    Eout = 0.0    \n",
    "    Ein = 0.0 \n",
    "    wSum = np.array([0.0,0.0,0.0]);\n",
    "    \n",
    "    for i in range(nExperimentos):\n",
    "\n",
    "        # gera funcao target F entre [-1, 1)\n",
    "        #targetFunctionF = np.random.rand(2)*2 - 1\n",
    "\n",
    "        # gera N pontos\n",
    "        innerDataX = np.random.rand(nPontos,2)*2 - 1\n",
    "        \n",
    "        innerY = np.array([])\n",
    "\n",
    "        # gera pontos em y -1 ou +1\n",
    "        for x in innerDataX:\n",
    "            innerY = np.append(innerY, classificaRegNLin(x))\n",
    "            \n",
    "        # gerando ruído\n",
    "        for i in range(quantidadeRuido):\n",
    "            #random float [0,1)\n",
    "            random = np.random.rand(1)[0]\n",
    "            \n",
    "            randomSignal = 1.0 if random > 0.5 else -1.0\n",
    "            \n",
    "            random2 = np.random.rand(1)[0]\n",
    "            # randomIndex = int [0, nPontos-1]\n",
    "            randomIndex = int((nPontos)*random2)\n",
    "            \n",
    "            innerY[randomIndex] = randomSignal\n",
    "            \n",
    "            \n",
    "        # fit em w e recebendo numero de iteracoes\n",
    "        fittedInnerW = regressionNLinFit(innerDataX, innerY)\n",
    "        #print(fittedInnerW)\n",
    "        wSum += fittedInnerW\n",
    "    \n",
    "        # gera N pontos para teste\n",
    "        innerTestDataX = np.random.rand(nPontos,2)*2 - 1\n",
    "        \n",
    "        # calculando numero de erros do conjunto de treinamento\n",
    "        errors = 0.0\n",
    "        for x in innerDataX:\n",
    "            predicted = regressionNLinPredict(fittedInnerW, x)\n",
    "            expected =  classificaRegNLin(x)\n",
    "            if predicted != expected:\n",
    "                errors = errors + 1.0\n",
    "        \n",
    "        innerEin = errors / nPontos;\n",
    "                        \n",
    "        # calculando numero de erros do conjunto de testes\n",
    "        errors = 0.0\n",
    "        for x in innerTestDataX:\n",
    "            predicted = regressionNLinPredict(fittedInnerW, x)\n",
    "            expected =  classificaRegNLin(x)\n",
    "            if predicted != expected:\n",
    "                errors = errors + 1.0\n",
    "        \n",
    "        innerEout = errors / nPontos;\n",
    "        \n",
    "        # adicionando Ein\n",
    "        Ein = Ein + innerEin\n",
    "        # adicionando Eout\n",
    "        Eout = Eout + innerEout\n",
    "            \n",
    "        \n",
    "    #end for\n",
    "    Eout = Eout/nExperimentos;\n",
    "    Ein = Ein/nExperimentos;\n",
    "    medianW = wSum/nExperimentos\n",
    "    \n",
    "    return Ein, Eout, medianW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Execute a Regressão Linear sem transformação usando o vetor de atributos:\n",
    "\n",
    "$(1, x_1, x_2)$, \n",
    "\n",
    "para encontrar o peso **w**. Qual é o valor aproximado de classificação do erro\n",
    "$E_{in}$ dentro da amostra? (Execute o experimento 100 vezes e use o valor médio de $E_{in}$ para reduzir a variação nos seus resultados.)\n",
    "\n",
    "a) $0$  b) $0.1$ **c) (x)** $0.3$ d) $0.5$ e) $0.8$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ein = 0.05178\n",
      "Eout = 0.055559999999999984\n",
      "Median W = [  7.89883015e-01   2.91855715e-03   1.18256340e+00  -1.11711349e-03\n",
      "  -1.26756896e+00  -7.88597974e-01]\n"
     ]
    }
   ],
   "source": [
    "Ein, Eout, midW = calculaRegressionNLin(1000, 100, 10)\n",
    "print('Ein = ' + str(Ein))\n",
    "print('Eout = ' + str(Eout))\n",
    "print('Median W = ' + str(midW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificaRegNLin(linha):\n",
    "  resultado = linha[0]**2 + linha[1]**2 - 0.6\n",
    "  \n",
    "  if (linha[1] >= resultado):\n",
    "    return 1.0\n",
    "  \n",
    "  return -1.0\n",
    "\n",
    "def regressionNLinFit(X, y):\n",
    "    trainingX = np.array([0,0,0,0,0,0])\n",
    "    \n",
    "    for x in X:\n",
    "        trainingX = np.vstack([trainingX, np.array([1, x[0], x[1], x[0]*x[1], x[0]**2, x[1]**2])])\n",
    "        \n",
    "    pseudoInvX = np.linalg.pinv(trainingX[1:])\n",
    "    return pseudoInvX.dot(y)\n",
    "\n",
    "\n",
    "def regressionNLinPredict(w, x):\n",
    "    regressionValue = w.T.dot(np.array([1, x[0], x[1], x[0]*x[1], x[0]**2, x[1]**2]))\n",
    "    return 1.0 if regressionValue >= 0 else -1.0\n",
    "\n",
    "def calculaRegressionNLin(nPontos, nExperimentos = 1000, quantidadeRuido = 10):\n",
    "    Eout = 0.0    \n",
    "    Ein = 0.0 \n",
    "    wSum = np.array([0.0,0.0,0.0,0.0,0.0,0.0]);\n",
    "    \n",
    "    for i in range(nExperimentos):\n",
    "\n",
    "        # gera funcao target F entre [-1, 1)\n",
    "        #targetFunctionF = np.random.rand(2)*2 - 1\n",
    "\n",
    "        # gera N pontos\n",
    "        innerDataX = np.random.rand(nPontos,2)*2 - 1\n",
    "        \n",
    "        innerY = np.array([])\n",
    "\n",
    "        # gera pontos em y -1 ou +1\n",
    "        for x in innerDataX:\n",
    "            innerY = np.append(innerY, classificaRegNLin(x))\n",
    "            \n",
    "        # gerando ruído\n",
    "        for i in range(quantidadeRuido):\n",
    "            #random float [0,1)\n",
    "            random = np.random.rand(1)[0]\n",
    "            \n",
    "            randomSignal = 1.0 if random > 0.5 else -1.0\n",
    "            \n",
    "            random2 = np.random.rand(1)[0]\n",
    "            # randomIndex = int [0, nPontos-1]\n",
    "            randomIndex = int((nPontos)*random2)\n",
    "            \n",
    "            innerY[randomIndex] = randomSignal\n",
    "            \n",
    "            \n",
    "        # fit em w e recebendo numero de iteracoes\n",
    "        fittedInnerW = regressionNLinFit(innerDataX, innerY)\n",
    "        #print(fittedInnerW)\n",
    "        \n",
    "    \n",
    "        # gera N pontos para teste\n",
    "        innerTestDataX = np.random.rand(nPontos,2)*2 - 1\n",
    "        \n",
    "        # calculando numero de erros do conjunto de treinamento\n",
    "        errors = 0.0\n",
    "        for x in innerDataX:\n",
    "            predicted = regressionNLinPredict(fittedInnerW, x)\n",
    "            expected =  classificaRegNLin(x)\n",
    "            if predicted != expected:\n",
    "                errors = errors + 1.0\n",
    "        \n",
    "        innerEin = errors / nPontos;\n",
    "                        \n",
    "        # calculando numero de erros do conjunto de testes\n",
    "        errors = 0.0\n",
    "        for x in innerTestDataX:\n",
    "            predicted = regressionNLinPredict(fittedInnerW, x)\n",
    "            expected =  classificaRegNLin(x)\n",
    "            if predicted != expected:\n",
    "                errors = errors + 1.0\n",
    "        \n",
    "        innerEout = errors / nPontos;\n",
    "        \n",
    "        # adicionando Ein\n",
    "        Ein = Ein + innerEin\n",
    "        # adicionando Eout\n",
    "        Eout = Eout + innerEout\n",
    "        # adicionando w\n",
    "        wSum = wSum + fittedInnerW\n",
    "            \n",
    "        \n",
    "    #end for\n",
    "    Eout = Eout/nExperimentos;\n",
    "    Ein = Ein/nExperimentos;\n",
    "    medianW = wSum/nExperimentos\n",
    "    \n",
    "    return Ein, Eout, medianW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Agora, transforme os N = 1000 dados de treinamento seguindo o vetor de\n",
    "atributos não-linear:\n",
    "\n",
    "$(1, x_1, x_2, x_1 x_2, x^2_1, x^2_2)$.\n",
    "\n",
    "\n",
    "Encontre o vetor $w_e$ que corresponde a solução da regressão linear. Quais das hipóteses a seguir é a mais próxima que você encontrou? Neste caso, próximo significa o valor que mais entra em acordo com sua hipótese (existe uma alta probabilidade de estar acordando com um ponto aleatoriamente selecionado). Em média algumas execuções serão necessárias para assegurar uma resposta estável.\n",
    "\n",
    "a) $g(x_1, x_2) = sign( -1 - 0.05x_1 + 0.08x_2 + 0.13x_1 x_2 + 1.5x^2_1 + 1.5x^2_2)$  \n",
    "b) $g(x_1, x_2) = sign( -1 - 0.05x_1 + 0.08x_2 + 0.13x_1 x_2 + 1.5x^2_1 + 15x^2_2)$  \n",
    "c) $g(x_1, x_2) = sign( -1 - 0.05x_1 + 0.08x_2 + 0.13x_1 x_2 + 15x^2_1 + 1.5x^2_2)  $  \n",
    "d) $g(x_1, x_2) = sign( -1 - 1.5x_1 + 0.08x_2 + 0.13x_1 x_2 + 0.05x^2_1 + 0.05x^2_2)  $  \n",
    "e) $g(x_1, x_2) = sign( -1 - 0.05x_1 + 0.08x_2 + 1.5x_1 x_2 + 0.15x^2_1 + 0.15x^2_2)  $  \n",
    "f) $Nenhuma$ **(x)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Qual o valor mais próximo do erro de classificação fora da amostra $E_{out}$ de sua hipótese no Problema 5? (Estime isso gerando um novo conjunto de 1000 pontos e adicione ruído, como antes. Em média 1000 execuções reduzem a variação em seus resultados).\n",
    "\n",
    "a) $0$ **b)(x)** $0.1$ c) $0.3$ d) $0.5$ e) $0.8$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ein = 0.047099999999999975\n",
      "Eout = 0.0681\n",
      "Median W = [ 0.77680229  0.00969515  1.21222684 -0.01438753 -1.23245675 -0.77486688]\n"
     ]
    }
   ],
   "source": [
    "Ein, Eout, midW = calculaRegressionNLin(100, 100, 1)\n",
    "print('Ein = ' + str(Ein))\n",
    "print('Eout = ' + str(Eout))\n",
    "print('Median W = ' + str(midW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "   Comparando com a implementação de um PLA, tem se a impressão que a implementação de uma regressão linear (ou logística) é um pouco mais complexa. Entretanto o tempo de execução parece ser menor quando comparado ao PLA. Uma contra-partida também da regressão linear é que existe a possibilidade de ocorrer Erros mesmo dentro dos dados de treinamento. \n",
    "   \n",
    "   Observou-se que a regressão linear pode ser bastante útil no aprendizado de funções target, porém as vezes é necessário fazer transformações com o dado de entrada para que esta seja eficiente. Essas transformações nem sempre são triviais e pode ser que sejam necessários testes para que se utilize a transformação mais apropriada dependendo da função target.\n",
    "   \n",
    "   Similarmente ao PLA, quanto maior $N$, menor a probabilidade de que $g(x_i) \\neq f(x_i)$ diminuindo assim tanto $E_{in}$ quanto $E_{out}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "ICTrab1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
